{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MM7Hm-U5ueb-"
   },
   "source": [
    "# Song-based Lyric Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char and word works ok-\n",
    "\n",
    "# models need to be saved\n",
    "\n",
    "# some code sections, such as generator can be generalized\n",
    "\n",
    "# NB: Several sections of code are copied, modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pycontractions\n",
      "  Downloading pycontractions-2.0.1-py3-none-any.whl (9.6 kB)\n",
      "Collecting gensim>=2.0\n",
      "  Downloading gensim-4.0.1-cp38-cp38-win_amd64.whl (23.9 MB)\n",
      "Collecting pyemd>=0.4.4\n",
      "  Downloading pyemd-0.5.1.tar.gz (91 kB)\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\bleli\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\bleli\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2v0rkxww\\\\pyemd_e336b2f918ab4f70973773506f1071cb\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\bleli\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2v0rkxww\\\\pyemd_e336b2f918ab4f70973773506f1071cb\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\bleli\\AppData\\Local\\Temp\\pip-wheel-s2_5ov7e'\n",
      "       cwd: C:\\Users\\bleli\\AppData\\Local\\Temp\\pip-install-2v0rkxww\\pyemd_e336b2f918ab4f70973773506f1071cb\\\n",
      "  Complete output (11 lines):\n",
      "  running bdist_wheel\n",
      "  running buildCollecting language-check>=1.0\n",
      "  Downloading language-check-1.1.tar.gz (33 kB)\n",
      "Collecting Cython==0.29.21\n",
      "  Downloading Cython-0.29.21-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\bleli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from gensim>=2.0->pycontractions) (1.6.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\bleli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from gensim>=2.0->pycontractions) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\bleli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from gensim>=2.0->pycontractions) (1.19.5)\n",
      "Requirement already satisfied: requests in c:\\users\\bleli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.25.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\bleli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\bleli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bleli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bleli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2020.12.5)\n",
      "Building wheels for collected packages: language-check, pyemd\n",
      "  Building wheel for language-check (setup.py): started\n",
      "  Building wheel for language-check (setup.py): finished with status 'done'\n",
      "  Created wheel for language-check: filename=language_check-1.1-py3-none-any.whl size=56968215 sha256=553ef92c85f804dfb14d353f7e40497c7a69218db5e3f240e020b80d174385cf\n",
      "  Stored in directory: c:\\users\\bleli\\appdata\\local\\pip\\cache\\wheels\\da\\c5\\78\\e19f0e3afc153a6fffa88abf7d12321a8f8b43e1faa3fc590b\n",
      "  Building wheel for pyemd (setup.py): started\n",
      "  Building wheel for pyemd (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pyemd\n",
      "Successfully built language-check\n",
      "Failed to build pyemd\n",
      "Installing collected packages: Cython, pyemd, language-check, gensim, pycontractions\n",
      "    Running setup.py install for pyemd: started\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.8\n",
      "  creating build\\lib.win-amd64-3.8\\pyemd\n",
      "  copying pyemd\\__about__.py -> build\\lib.win-amd64-3.8\\pyemd\n",
      "  copying pyemd\\__init__.py -> build\\lib.win-amd64-3.8\\pyemd\n",
      "  running build_ext\n",
      "  building 'pyemd.emd' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for pyemd\n",
      "  WARNING: The scripts cygdb.exe, cython.exe and cythonize.exe are installed in 'C:\\Users\\bleli\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\bleli\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\bleli\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2v0rkxww\\\\pyemd_e336b2f918ab4f70973773506f1071cb\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\bleli\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2v0rkxww\\\\pyemd_e336b2f918ab4f70973773506f1071cb\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\bleli\\AppData\\Local\\Temp\\pip-record-8tmpfmr_\\install-record.txt' --single-version-externally-managed --user --prefix= --compile --install-headers 'C:\\Users\\bleli\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\Include\\pyemd'\n",
      "         cwd: C:\\Users\\bleli\\AppData\\Local\\Temp\\pip-install-2v0rkxww\\pyemd_e336b2f918ab4f70973773506f1071cb\\\n",
      "    Complete output (15 lines):\n",
      "    Compiling pyemd/emd.pyx because it depends on C:\\Users\\bleli\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\Cython\\Includes\\libcpp\\vector.pxd.\n",
      "    [1/1] Cythonizing pyemd/emd.pyx\n",
      "    C:\\Users\\bleli\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\bleli\\AppData\\Local\\Temp\\pip-install-2v0rkxww\\pyemd_e336b2f918ab4f70973773506f1071cb\\pyemd\\emd.pyx\n",
      "      tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.8\n",
      "    creating build\\lib.win-amd64-3.8\\pyemd\n",
      "    copying pyemd\\__about__.py -> build\\lib.win-amd64-3.8\\pyemd\n",
      "    copying pyemd\\__init__.py -> build\\lib.win-amd64-3.8\\pyemd\n",
      "    running build_ext\n",
      "    building 'pyemd.emd' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\bleli\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\bleli\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2v0rkxww\\\\pyemd_e336b2f918ab4f70973773506f1071cb\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\bleli\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2v0rkxww\\\\pyemd_e336b2f918ab4f70973773506f1071cb\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\bleli\\AppData\\Local\\Temp\\pip-record-8tmpfmr_\\install-record.txt' --single-version-externally-managed --user --prefix= --compile --install-headers 'C:\\Users\\bleli\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\Include\\pyemd' Check the logs for full command output.\n",
      "WARNING: You are using pip version 21.0.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\bleli\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n",
      "\n",
      "    Running setup.py install for pyemd: finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "!pip install pycontractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbYkbSTYeJ5A",
    "outputId": "0aa63251-7058-4c44-d6d0-c36a8a9b4356"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycontractions'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-464705a9c41b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mku\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpycontractions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensorflow Version: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycontractions'"
     ]
    }
   ],
   "source": [
    "#@title Libraries\n",
    "#!pip install tensorflow\n",
    "#!pip install nltk\n",
    "#!pip install contractions\n",
    "#nltk.download('punkt')\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku\n",
    "from pycontractions import contractions\n",
    "\n",
    "print('Tensorflow Version: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycontractions'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a2300602cd3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpycontractions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycontractions'"
     ]
    }
   ],
   "source": [
    "import pycontractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yfnSPZQZe0yv"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Unnamed: 0 Unnamed: 0.1               title                date       album  \\\n",
       "0          0            0           Let It Be         May 8, 1970   Let It Be   \n",
       "1          1            1       Come Together  September 26, 1969  Abbey Road   \n",
       "2          2            2           Yesterday  September 13, 1965  Help! (UK)   \n",
       "3          3            3           Something  September 26, 1969  Abbey Road   \n",
       "4          4            4  Here Comes the Sun  September 26, 1969  Abbey Road   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  \\r\\n\\r\\n[Verse 1]\\r\\nWhen I find myself in tim...   \n",
       "1  \\r\\n\\r\\n[Intro]\\r\\nShoot me\\r\\nShoot me\\r\\nSho...   \n",
       "2  \\r\\n\\r\\n[Verse 1]\\r\\nYesterday\\r\\nAll my troub...   \n",
       "3  \\r\\n\\r\\n[Verse 1]\\r\\nSomething in the way she ...   \n",
       "4  \\r\\n\\r\\n[Chorus]\\r\\nHere comes the sun, doo da...   \n",
       "\n",
       "                                               intro  \\\n",
       "0                                                 []   \n",
       "1  ['[Intro]\\nShoot me\\nShoot me\\nShoot me\\nShoot...   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                               verse  \\\n",
       "0  ['[Verse 1]\\nWhen I find myself in times of tr...   \n",
       "1  [\"[Verse 1]\\nHere come old flat-top, he come g...   \n",
       "2  [\"[Verse 1]\\nYesterday\\nAll my troubles seemed...   \n",
       "3  ['[Verse 1]\\nSomething in the way she moves\\nA...   \n",
       "4  [\"[Verse 1]\\nLittle darling, it's been a long ...   \n",
       "\n",
       "                                              chorus  \\\n",
       "0  ['[Chorus]\\nLet it be, let it be, let it be, l...   \n",
       "1  ['[Chorus]\\nCome together, right now\\nOver me'...   \n",
       "2  [\"[Chorus]\\nWhy she had to go\\nI don't know, s...   \n",
       "3  [\"[Chorus]\\nI don't want to leave her now\\nYou...   \n",
       "4  [\"[Chorus]\\nHere comes the sun, doo da doo doo...   \n",
       "\n",
       "                                              bridge  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3  [\"[Bridge]\\nYou're asking me will my love grow...   \n",
       "4  ['[Bridge]\\nSun, sun, sun, here it comes\\nSun,...   \n",
       "\n",
       "                                               outro  \\\n",
       "0                                                 []   \n",
       "1  ['[Outro]\\nCome together, yeah\\nCome together,...   \n",
       "2          ['[Outro]\\nMmm-mmm-mmm-mmm-mmm, hmm-hmm']   \n",
       "3  [\"[Outro]\\nI don't want to leave her now\\nYou ...   \n",
       "4  [\"[Outro]\\nHere comes the sun, doo da doo doo\\...   \n",
       "\n",
       "                           intro_cleaned  \\\n",
       "0                                    NaN   \n",
       "1   shoot me shoot me shoot me shoot me'   \n",
       "2                                    NaN   \n",
       "3                                    NaN   \n",
       "4                                    NaN   \n",
       "\n",
       "                                       verse_cleaned  \\\n",
       "0   when i find myself in times of trouble, mothe...   \n",
       "1   here come old flat-top, he come groovin' up s...   \n",
       "2   yesterday all my troubles seemed so far away ...   \n",
       "3   something in the way she moves attracts me li...   \n",
       "4   little darling, it's been a long cold lonely ...   \n",
       "\n",
       "                                      bridge_cleaned  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3   you're asking me will my love grow i don't kn...   \n",
       "4   sun, sun, sun, here it comes sun, sun, sun, h...   \n",
       "\n",
       "                                      chorus_cleaned  \\\n",
       "0   let it be, let it be, let it be, let it be wh...   \n",
       "1   come together, right now over me', come toget...   \n",
       "2   why she had to go i don't know, she wouldn't ...   \n",
       "3   i don't want to leave her now you know i beli...   \n",
       "4   here comes the sun, doo da doo doo here comes...   \n",
       "\n",
       "                                       outro_cleaned  \n",
       "0                                                NaN  \n",
       "1   come together, yeah come together, yeah come ...  \n",
       "2                      mmm-mmm-mmm-mmm-mmm, hmm-hmm'  \n",
       "3   i don't want to leave her now you know i beli...  \n",
       "4   here comes the sun, doo da doo doo here comes...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 0.1</th>\n      <th>title</th>\n      <th>date</th>\n      <th>album</th>\n      <th>lyrics</th>\n      <th>intro</th>\n      <th>verse</th>\n      <th>chorus</th>\n      <th>bridge</th>\n      <th>outro</th>\n      <th>intro_cleaned</th>\n      <th>verse_cleaned</th>\n      <th>bridge_cleaned</th>\n      <th>chorus_cleaned</th>\n      <th>outro_cleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>Let It Be</td>\n      <td>May 8, 1970</td>\n      <td>Let It Be</td>\n      <td>\\r\\n\\r\\n[Verse 1]\\r\\nWhen I find myself in tim...</td>\n      <td>[]</td>\n      <td>['[Verse 1]\\nWhen I find myself in times of tr...</td>\n      <td>['[Chorus]\\nLet it be, let it be, let it be, l...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>NaN</td>\n      <td>when i find myself in times of trouble, mothe...</td>\n      <td>NaN</td>\n      <td>let it be, let it be, let it be, let it be wh...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Come Together</td>\n      <td>September 26, 1969</td>\n      <td>Abbey Road</td>\n      <td>\\r\\n\\r\\n[Intro]\\r\\nShoot me\\r\\nShoot me\\r\\nSho...</td>\n      <td>['[Intro]\\nShoot me\\nShoot me\\nShoot me\\nShoot...</td>\n      <td>[\"[Verse 1]\\nHere come old flat-top, he come g...</td>\n      <td>['[Chorus]\\nCome together, right now\\nOver me'...</td>\n      <td>[]</td>\n      <td>['[Outro]\\nCome together, yeah\\nCome together,...</td>\n      <td>shoot me shoot me shoot me shoot me'</td>\n      <td>here come old flat-top, he come groovin' up s...</td>\n      <td>NaN</td>\n      <td>come together, right now over me', come toget...</td>\n      <td>come together, yeah come together, yeah come ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>Yesterday</td>\n      <td>September 13, 1965</td>\n      <td>Help! (UK)</td>\n      <td>\\r\\n\\r\\n[Verse 1]\\r\\nYesterday\\r\\nAll my troub...</td>\n      <td>[]</td>\n      <td>[\"[Verse 1]\\nYesterday\\nAll my troubles seemed...</td>\n      <td>[\"[Chorus]\\nWhy she had to go\\nI don't know, s...</td>\n      <td>[]</td>\n      <td>['[Outro]\\nMmm-mmm-mmm-mmm-mmm, hmm-hmm']</td>\n      <td>NaN</td>\n      <td>yesterday all my troubles seemed so far away ...</td>\n      <td>NaN</td>\n      <td>why she had to go i don't know, she wouldn't ...</td>\n      <td>mmm-mmm-mmm-mmm-mmm, hmm-hmm'</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>Something</td>\n      <td>September 26, 1969</td>\n      <td>Abbey Road</td>\n      <td>\\r\\n\\r\\n[Verse 1]\\r\\nSomething in the way she ...</td>\n      <td>[]</td>\n      <td>['[Verse 1]\\nSomething in the way she moves\\nA...</td>\n      <td>[\"[Chorus]\\nI don't want to leave her now\\nYou...</td>\n      <td>[\"[Bridge]\\nYou're asking me will my love grow...</td>\n      <td>[\"[Outro]\\nI don't want to leave her now\\nYou ...</td>\n      <td>NaN</td>\n      <td>something in the way she moves attracts me li...</td>\n      <td>you're asking me will my love grow i don't kn...</td>\n      <td>i don't want to leave her now you know i beli...</td>\n      <td>i don't want to leave her now you know i beli...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>Here Comes the Sun</td>\n      <td>September 26, 1969</td>\n      <td>Abbey Road</td>\n      <td>\\r\\n\\r\\n[Chorus]\\r\\nHere comes the sun, doo da...</td>\n      <td>[]</td>\n      <td>[\"[Verse 1]\\nLittle darling, it's been a long ...</td>\n      <td>[\"[Chorus]\\nHere comes the sun, doo da doo doo...</td>\n      <td>['[Bridge]\\nSun, sun, sun, here it comes\\nSun,...</td>\n      <td>[\"[Outro]\\nHere comes the sun, doo da doo doo\\...</td>\n      <td>NaN</td>\n      <td>little darling, it's been a long cold lonely ...</td>\n      <td>sun, sun, sun, here it comes sun, sun, sun, h...</td>\n      <td>here comes the sun, doo da doo doo here comes...</td>\n      <td>here comes the sun, doo da doo doo here comes...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_dataset.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxelBhzWP2Vt"
   },
   "source": [
    "**Clean Lyrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "8n1s_OpMu2Yb"
   },
   "outputs": [],
   "source": [
    "#@title Clean Lyrics\n",
    "def clean(s):\n",
    "    \n",
    "    # Expand contractions\n",
    "    s = s.split()\n",
    "    #new_s = []\n",
    "    #for word in s:\n",
    "        #new_s.append(contractions.fix(word))\n",
    "    #s = \" \".join(new_s)\n",
    "\n",
    "    # Format words and remove unwanted characters\n",
    "    s = re.sub(r'[\\(\\[].*?[\\)\\]] +', '', s)\n",
    "    s = re.sub(r' [\\(\\[].*?[\\)\\]]', '', s)\n",
    "    s = re.sub(r\"[^’'a-zA-Z \\ ]+\", ' ', s)\n",
    "    s = re.sub(' +', ' ', s)\n",
    "    s = s.lower()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PMJyTYOwA9pH"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a29a9ec189c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# new colunn with clean lyrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#df['lyrics_cleaned'] = list(map(clean, df.lyrics))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlyrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#df['test'] = df['lyrics_cleaned'][310:311]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#df['lyrics_cleaned'] = df['lyrics_cleaned'][0:310]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-367ddcbd1f24>\u001b[0m in \u001b[0;36mclean\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Format words and remove unwanted characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[\\(\\[].*?[\\)\\]] +'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr' [\\(\\[].*?[\\)\\]]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"[^’'a-zA-Z \\ ]+\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# new colunn with clean lyrics\n",
    "df['lyrics_cleaned'] = list(map(clean, df.lyrics))\n",
    "df['test'] = df['lyrics_cleaned'][310:311]\n",
    "df['lyrics_cleaned'] = df['lyrics_cleaned'][0:310]\n",
    "lyrics = df['lyrics_cleaned'].dropna()\n",
    "test = df['test'].dropna()\n",
    "test.reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "cellView": "form",
    "id": "rmpInsN2B_fB"
   },
   "outputs": [],
   "source": [
    "#@title Extract Text\n",
    "# extract all from lyrics cleaned col to string\n",
    "# todo: regex remove all stopchars, puncation etc, except: [Intro], [Verse], [Chorus], hence remove verse number ([Verse 1])\n",
    "def extract_text(lyrics):\n",
    "    global corpus\n",
    "    lst = []\n",
    "    for i in range(len(lyrics)):\n",
    "        lst.append(str(lyrics[i]))\n",
    "    data = ''.join(lst)\n",
    "    \n",
    "    corpus = []\n",
    "    corpus += [w for w in data.split(' ') if w.strip() != '' or w == '\\n']\n",
    "    lyrics = ' '.join(corpus)\n",
    "    \n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ar3vlhG2CBCe",
    "outputId": "2166f70c-5be0-406e-9ee8-cf5befd8dabc"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Expand contractions\n",
    "def expand(s): \n",
    "    s = s.lower()\n",
    "    s = s.split()\n",
    "    new_s = []\n",
    "    for word in s:\n",
    "        new_s.append(contractions.fix(word))\n",
    "    s = \" \".join(new_s)\n",
    "    return s\n",
    "\n",
    "lyrics = extract_text(lyrics)\n",
    "#lyrics = expand(lyrics)\n",
    "intro = extract_text(df['intro_cleaned'])\n",
    "#intro = expand(intro)\n",
    "verse = extract_text(df['verse_cleaned'])\n",
    "#verse = expand(verse)\n",
    "chorus = extract_text(df['chorus_cleaned'])\n",
    "#chorus = expand(chorus)\n",
    "bridge = extract_text(df['bridge_cleaned'])\n",
    "#bridge = expand(bridge)\n",
    "outro = extract_text(df['outro_cleaned'])\n",
    "#outro = expand(outro)\n",
    "test = extract_text(test)\n",
    "#test = expand(test)\n",
    "\n",
    "# most common words from string\n",
    "Counter_intro = Counter(intro.split())\n",
    "most_occur_intro = Counter_intro.most_common(10)\n",
    "\n",
    "Counter_verse = Counter(verse.split())\n",
    "most_occur_verse = Counter_verse.most_common(20)\n",
    "\n",
    "Counter_chorus = Counter(chorus.split())\n",
    "most_occur_chorus = Counter_chorus.most_common(20)\n",
    "\n",
    "Counter_bridge = Counter(bridge.split())\n",
    "most_occur_bridge = Counter_bridge.most_common(20)\n",
    "\n",
    "Counter_outro = Counter(outro.split())\n",
    "most_occur_outro = Counter_outro.most_common(10)\n",
    "\n",
    "#random.choice(most_occur_verse)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bmloyk3fP5m3"
   },
   "source": [
    "**Get Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "4p4iBYgh19QA"
   },
   "outputs": [],
   "source": [
    "# Choose characted based or word based text generation\n",
    "char_ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sshbGjbKwrQ9",
    "outputId": "21a0dd97-2b77-446e-834a-687ac0f9eb49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 unique characters in vocabulary\n",
      "<class 'str'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#@title Get Vocabulary\n",
    "# vocab char/word [T/F] (default = T)\n",
    "def vocab(lyrics, char = char_): #might do global char to specify type in beginning\n",
    "    if char == True:\n",
    "        vocab = sorted(set(lyrics))\n",
    "        print(f'{len(vocab)} unique characters in vocabulary')\n",
    "    else:\n",
    "        lyrics = [w for w in lyrics.split(' ')]\n",
    "        vocab = sorted(set(lyrics))\n",
    "        print(f'{len(vocab)} unique words in vocabulary')\n",
    "    return lyrics, vocab\n",
    "\n",
    "lyrics, vocab = vocab(lyrics)\n",
    "\n",
    "print(type(lyrics)); print(type(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjLKKhk3QJBR"
   },
   "source": [
    "**Vectorize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "cellView": "form",
    "id": "ML-xuiXwQZ9X"
   },
   "outputs": [],
   "source": [
    "#@title Vectorize\n",
    "# vec char/word [T/F] (default = T)\n",
    "## keep this structure for now, reduce later\n",
    "def vec_data(vocab, lyrics, char = char_):\n",
    "    if char == True: \n",
    "        print('character-based')\n",
    "        char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "        idx2char = np.array(vocab)\n",
    "        text_as_int = np.array([char2idx[c] for c in lyrics])\n",
    "        return char2idx, idx2char, text_as_int\n",
    "\n",
    "    else:\n",
    "        word2idx = {u:i for i, u in enumerate(vocab)}\n",
    "        print('word-based')\n",
    "        idx2words = np.array(vocab)\n",
    "        word_as_int = np.array([word2idx[w] for w in lyrics])\n",
    "        return word2idx, idx2words, word_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yPrUfiQyWsDh",
    "outputId": "c4c77f1a-cb45-4b23-847f-73ef63fb7bcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character-based\n",
      "<class 'dict'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "29\n",
      "29\n",
      "328295\n"
     ]
    }
   ],
   "source": [
    "# call based on char [T/F]\n",
    "cw_2idx, idx2_cw, as_int = vec_data(vocab, lyrics, char = char_)\n",
    "\n",
    "#%%\n",
    "print(type(cw_2idx)); print(type(idx2_cw)); print(type(as_int))\n",
    "print(len(cw_2idx)); print(len(idx2_cw)); print(len(as_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YkL4qNYc1uL",
    "outputId": "f97169e4-283b-45f8-bf49-102d93f795e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length set to: 100\n",
      "Examples per epoch set to: 3250\n"
     ]
    }
   ],
   "source": [
    "#@title Set Sequence\n",
    "#define sequence\n",
    "def seq(lyrics = lyrics, char = char_):\n",
    "    global seq_length\n",
    "    global examples_per_epoch\n",
    "    if char == True:\n",
    "        seq_length = 100\n",
    "        examples_per_epoch = len(lyrics)//(seq_length+1)\n",
    "    else: \n",
    "        seq_length = 10\n",
    "        examples_per_epoch = len(corpus)//(seq_length+1)\n",
    "\n",
    "#call seq\n",
    "seq()\n",
    "\n",
    "print(f'Sequence length set to: {seq_length}')\n",
    "print(f'Examples per epoch set to: {examples_per_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SKAXaibk7zk"
   },
   "source": [
    "**Set Model Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "G7T0x-8Jh31Q"
   },
   "outputs": [],
   "source": [
    "cw_dataset = tf.data.Dataset.from_tensor_slices(as_int)\n",
    "sequences = cw_dataset.batch(seq_length+1, drop_remainder=True) # generating batches of 10 words each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "M1zUK5N0h4kG"
   },
   "outputs": [],
   "source": [
    "#further preprossing (duplicate and shift one step below) (helper function)\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zf6s0vVnisv5",
    "outputId": "7ee1c9b1-310d-45b1-d2fe-6ff26f2d7101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# define batch & buffer size\n",
    "batch_size = 64\n",
    "buffer_size = 100 #this should be changed with char/word, consider putting it in parameter function\n",
    "\n",
    "steps_per_epoch =  len(lyrics) // batch_size\n",
    "\n",
    "#shuffle data\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True) #.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# set parameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "# check shapes\n",
    "print(dataset) # should be ((int, int), (int, int)), else run from top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw65MW1Xm0e0"
   },
   "source": [
    "**Create Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "S6dzqp93y5r2"
   },
   "outputs": [],
   "source": [
    "def createModel(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMCBB4NOBTYY",
    "outputId": "901056e1-b7c7-4cd0-bf5a-74ac60868bdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Character based Model: \n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (64, None, 256)           7424      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 29)            29725     \n",
      "=================================================================\n",
      "Total params: 3,975,453\n",
      "Trainable params: 3,975,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if char_ == True:\n",
    "    model_char = createModel(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)\n",
    "    print('Character based Model: ')\n",
    "    model_char.summary()\n",
    "else:\n",
    "    model_word = createModel(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)\n",
    "    print('Word based Model: ')\n",
    "    model_word.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trlmfmX_oY51"
   },
   "source": [
    "**Set Loss Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "XKbJZF38z1Ou",
    "outputId": "c261998a-f3ee-49d8-eb23-a7211f44cdbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 41s 795ms/step - loss: 3.0889\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 40s 798ms/step - loss: 2.2426\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 40s 792ms/step - loss: 2.0438\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 39s 789ms/step - loss: 1.9157\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 40s 792ms/step - loss: 1.8066\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 40s 797ms/step - loss: 1.7079\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 40s 795ms/step - loss: 1.6195\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 39s 789ms/step - loss: 1.5404\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 40s 800ms/step - loss: 1.4671\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 40s 793ms/step - loss: 1.4012\n",
      "400.54 seconds running time for char-based model\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "\n",
    "if char_ == False:\n",
    "    EPOCHS = 20\n",
    "    start_word = time.time()\n",
    "    model_word.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "    #save training checkpoints\n",
    "    checkpoint_dir = './training_checkpoints_word'\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "    checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_prefix,\n",
    "        save_weights_only=True)\n",
    "    \n",
    "    history = model_word.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "    tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    model_word = create_model(len(vocab), embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "    model_word.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    model_word.build(tf.TensorShape([1, None]))\n",
    "    end_word = time.time()\n",
    "    print(round(end_word-start_word, 2), 'seconds running time for word-based model')\n",
    "    \n",
    "    \n",
    "\n",
    "elif char_ == True:\n",
    "    \n",
    "    EPOCHS = 10\n",
    "    start_char = time.time()\n",
    "    model_char.compile(optimizer='adam', loss=loss)\n",
    "    \n",
    "    #save training checkpoints\n",
    "    checkpoint_dir = './training_checkpoints_char'\n",
    "    \n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "    checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_prefix,\n",
    "        save_weights_only=True)\n",
    "    \n",
    "    history = model_char.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "    tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    model_char = createModel(len(vocab), embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "    model_char.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    model_char.build(tf.TensorShape([1, None]))\n",
    "    end_char = time.time()\n",
    "    print(round(end_char-start_char, 2), 'seconds running time for char-based model')\n",
    "\n",
    "else: \n",
    "    print('Set char_ to T/F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "M0Qd_SI-9sk7",
    "outputId": "8ee9e2cf-b615-46a2-e555-1b4d2aa87086"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e4f03981d906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#save trained model for future use (so we do not have to train it every time we want to generate text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Example:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerateLyrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartString\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu\"love\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_word' is not defined"
     ]
    }
   ],
   "source": [
    "def generateLyrics(model, startString, temp, num_generate = 30):\n",
    "    #print(\"---- Generating lyrics starting with '\" + startString + \"' ----\")\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    start_string_list =  [w for w in startString.split(' ')]\n",
    "    input_eval = [cw_2idx[s] for s in start_string_list]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # temp represent how 'conservative' the predictions are. \n",
    "        # Lower temp leads to more predictable (or correct) lyrics\n",
    "        predictions = predictions / temp \n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(' ' + idx2_cw[predicted_id])\n",
    "\n",
    "    return (startString + ''.join(text_generated))\n",
    "\n",
    "#save trained model for future use (so we do not have to train it every time we want to generate text)\n",
    "model_word.save('saved_model.h5') \n",
    "print(\"Example:\")\n",
    "print(generateLyrics(model_word, startString=u\"love\", temp=0.6))\n",
    "while (True):\n",
    "    print('Enter start string:')\n",
    "    input_str = input().lower().strip()\n",
    "    print('Enter temp:')\n",
    "    temp = float(input())\n",
    "    print(generateLyrics(model_word, startString=input_str, temp=temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_song(format,model):\n",
    "    \n",
    "    temp = 0.6\n",
    "    song = []\n",
    "    \n",
    "    if model == model_char:\n",
    "        a,b,c,d,e = 200,300,300,200,300\n",
    "        generator = char_based_generator\n",
    "    else: \n",
    "        a,b,c,d,e = 20,30,30,20,30\n",
    "        generator = generateLyrics\n",
    "        \n",
    "    intro = generator(model_char, startString=random.choice(most_occur_intro)[0], temp=temp, num_generate = a)\n",
    "    chorus = generator(model_char, startString=random.choice(most_occur_chorus)[0], temp=temp, num_generate = b)\n",
    "    bridge = generator(model_char, startString=random.choice(most_occur_bridge)[0], temp=temp, num_generate = c)\n",
    "    outro = generator(model_char, startString=random.choice(most_occur_outro)[0], temp=temp, num_generate = d)\n",
    "    \n",
    "    for tag in format:\n",
    "        if tag == \"I\":\n",
    "            print('[INTRO]\\n',intro,'\\n')\n",
    "        elif tag == \"V\":\n",
    "            verse = generator(model_char, startString=random. choice(most_occur_verse)[0], temp=temp, num_generate = e)\n",
    "            print('[VERSE]\\n',verse,'\\n')\n",
    "        elif tag == \"B\":\n",
    "            print('[BRIDGE]\\n',bridge,'\\n')\n",
    "        elif tag == \"C\":\n",
    "            print('[CHORUS]\\n',chorus,'\\n')\n",
    "        elif tag == \"O\":\n",
    "            print('[OUTRO]\\n',outro)\n",
    "        else: \n",
    "            raise ValueError('wrong input')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INTRO]\n",
      " you here home in am we are going to carry you want me the wait. i cull enged me to you the waiting but ev'rybody do not you know i think it is go on the benthow but i am not no don ever have here all you \n",
      "\n",
      "[VERSE]\n",
      " is bempmony lead me the wanter have been plone to the you one sweet donn and i do not know what a said make me starting then it all got to know something rack of the world do not want me say he will have here it is not the end of the sun and in the meanie conds. we all gotether now a to calling of bea \n",
      "\n",
      "[CHORUS]\n",
      " to you do not know my love to the thought the world the party called me no i no not no do not know know show my knee when you can not you to be a uppering to the singers off i need so i will not the pain out a seace the ring back to well you will get you one can not going to the way a pain easter you  \n",
      "\n",
      "[VERSE]\n",
      " a hand masic finger dim not every the ligsten but a pooplow now i need it is come so hall hand not girl you know that i have and i do not want to be a people people end the.. i am going to you you better help me well pred we have been gone that i me mine there is you want me to lose that girl you wou \n",
      "\n",
      "[BRIDGE]\n",
      " that man the was the window und you know the new yeah yeah yeah again. every happy you are going to lose that said so i can not you know i need you being around help me no do not say that. me i go and rown of the sund of the world if you so if i am said you sill but she have got me help me well i love y \n",
      "\n",
      "[CHORUS]\n",
      " to you do not know my love to the thought the world the party called me no i no not no do not know know show my knee when you can not you to be a uppering to the singers off i need so i will not the pain out a seace the ring back to well you will get you one can not going to the way a pain easter you  \n",
      "\n",
      "[VERSE]\n",
      " not before that are going to change my mman back he is got to to do do do do do do do do do do do do do look at the nawfer number time it is near you see you bet me got to go so i got to ride she ought to fide a lous and make me you are got a ticket the lonely pait and i will never weep to go i am goin \n",
      "\n",
      "[OUTRO]\n",
      " ally heart if you till the way my love will baby not know how to the hands in a yeah oh yeah you know my nane had a was a gir me to the ring a point of how i need as to make you love to to site to help m\n"
     ]
    }
   ],
   "source": [
    "song = make_song(\"IVCVBCVO\",model_char)\n",
    "song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSE0e_GDpCwb"
   },
   "source": [
    "**Lyric Generators**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TCI06my7KkUT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "hey happy we want me so in. it ev\n"
     ]
    }
   ],
   "source": [
    "# The prediction loop\n",
    "def char_based_generator(model, startString, temp, num_generate = 30):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    " \n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [cw_2idx[s] for s in startString]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = temp\n",
    " \n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2_cw[predicted_id])\n",
    " \n",
    "    return (startString + ''.join(text_generated))\n",
    "model_char.save('char_based_model.h5') \n",
    "print(char_based_generator(model_char, startString=u\"hey\",temp = 0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_loss_by_example(logits,\n",
    "                             targets,\n",
    "                             weights,\n",
    "                             average_across_timesteps=True,\n",
    "                             softmax_loss_function=None,\n",
    "                             name=None):\n",
    "  \"\"\"Weighted cross-entropy loss for a sequence of logits (per example).\n",
    "  Args:\n",
    "    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n",
    "    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n",
    "    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n",
    "    average_across_timesteps: If set, divide the returned cost by the total\n",
    "      label weight.\n",
    "    softmax_loss_function: Function (labels-batch, inputs-batch) -> loss-batch\n",
    "      to be used instead of the standard softmax (the default if this is None).\n",
    "    name: Optional name for this operation, default: \"sequence_loss_by_example\".\n",
    "  Returns:\n",
    "    1D batch-sized float Tensor: The log-perplexity for each sequence.\n",
    "  Raises:\n",
    "    ValueError: If len(logits) is different from len(targets) or len(weights).\n",
    "  \"\"\"\n",
    "      if len(targets) != len(logits) or len(weights) != len(logits):\n",
    "          raise ValueError(\"Lengths of logits, weights, and targets must be the same \"\n",
    "                     \"%d, %d, %d.\" % (len(logits), len(weights), len(targets)))\n",
    "    with ops.name_scope(name, \"sequence_loss_by_example\",\n",
    "                      logits + targets + weights):\n",
    "    log_perp_list = []\n",
    "    for logit, target, weight in zip(logits, targets, weights):\n",
    "      if softmax_loss_function is None:\n",
    "        # TODO(irving,ebrevdo): This reshape is needed because\n",
    "        # sequence_loss_by_example is called with scalars sometimes, which\n",
    "        # violates our general scalar strictness policy.\n",
    "        target = array_ops.reshape(target, [-1])\n",
    "        crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=target, logits=logit)\n",
    "      else:\n",
    "        crossent = softmax_loss_function(target, logit)\n",
    "      log_perp_list.append(crossent * weight)\n",
    "    log_perps = math_ops.add_n(log_perp_list)\n",
    "    if average_across_timesteps:\n",
    "      total_size = math_ops.add_n(weights)\n",
    "      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n",
    "      log_perps /= total_size\n",
    "  return log_perps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset is equal to test from the beginning.\n",
    "#this should work for n-grams. \n",
    "#need a big dictionary with the probability for each word. aka. model[word]\n",
    "\n",
    "def perplexity(testset, model)\n",
    "    testset = testset.split()\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for word in testset:\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/model[word])\n",
    "    perplexity = pow(perplexity, 1/float(N)) \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-244-93ea7c4ed4cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "logits = keras.layers.Dense(10)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fbc9f0f2c10>) with an unsupported type (<class 'tensorflow.python.keras.engine.sequential.Sequential'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-a36a8b53281c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_v2\u001b[0;34m(logits, axis, name)\u001b[0m\n\u001b[1;32m   3699\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3700\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3701\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_wrap_2d_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_wrap_2d_function\u001b[0;34m(inputs, compute_op, dim, name)\u001b[0m\n\u001b[1;32m   3611\u001b[0m         name=name)\n\u001b[1;32m   3612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3613\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m   \u001b[0;31m# We need its original shape for shape inference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fbc9f0f2c10>) with an unsupported type (<class 'tensorflow.python.keras.engine.sequential.Sequential'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "probabilities = tf.nn.softmax(model_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Sequential' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-cf2e352e96e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-234-f15018ce15b4>\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(testset, model)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperplexity\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Sequential' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "perplexity(test, model_char)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BLG.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "24222ea5e13fc1c5b7be465c1e49a9583458cf9304a3c5bef97b23d1aa1f4b2c"
   }
  },
  "interpreter": {
   "hash": "24222ea5e13fc1c5b7be465c1e49a9583458cf9304a3c5bef97b23d1aa1f4b2c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}