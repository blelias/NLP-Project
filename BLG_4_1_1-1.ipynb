{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MM7Hm-U5ueb-"
   },
   "source": [
    "# Song-based Lyric Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char and word works ok-\n",
    "\n",
    "# models need to be saved\n",
    "\n",
    "# some code sections, such as generator can be generalized\n",
    "\n",
    "# NB: Several sections of code are copied, modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbYkbSTYeJ5A",
    "outputId": "0aa63251-7058-4c44-d6d0-c36a8a9b4356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
      "\u001b[K     |█████████████████████           | 297.3 MB 144.3 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 454.3 MB 30 kB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.12.4)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.10.0)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 30.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.8 MB 29.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 249 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 36.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.7.4.2)\n",
      "Collecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 32.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 33.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 27.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 603 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.9.2->tensorflow) (49.6.0.post20200814)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow) (1.21.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 33.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow) (2.24.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 33.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cached-property; python_version < \"3.8\"\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Building wheels for collected packages: wrapt, termcolor\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=77167 sha256=d6acc9d410077a30793ff8f2553489a13bdf1ba278eae1fa79e08e15e2153ec7\n",
      "  Stored in directory: /home/ucloud/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=717a9911ecc85765c7efc1cea9d9ded214a9814eaf807a9f9f5ae365a1b0de89\n",
      "  Stored in directory: /home/ucloud/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built wrapt termcolor\n",
      "Installing collected packages: flatbuffers, wrapt, astunparse, keras-nightly, numpy, keras-preprocessing, tensorflow-estimator, grpcio, tensorboard-data-server, tensorboard-plugin-wit, tensorboard, cached-property, h5py, termcolor, opt-einsum, gast, google-pasta, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.31.0\n",
      "    Uninstalling grpcio-1.31.0:\n",
      "      Successfully uninstalled grpcio-1.31.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.1.0\n",
      "    Uninstalling tensorboard-2.1.0:\n",
      "      Successfully uninstalled tensorboard-2.1.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "Successfully installed astunparse-1.6.3 cached-property-1.5.2 flatbuffers-1.12 gast-0.4.0 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 numpy-1.19.5 opt-einsum-3.3.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 wrapt-1.12.1\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from nltk) (2020.7.14)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.48.2)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.6.2\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.0.49-py2.py3-none-any.whl (6.4 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
      "\u001b[K     |████████████████████████████████| 321 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting anyascii\n",
      "  Downloading anyascii-0.2.0-py3-none-any.whl (283 kB)\n",
      "\u001b[K     |████████████████████████████████| 283 kB 15.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
      "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=106975 sha256=9fc16694464a351859ae54e88ce49c13c59b7c032f2a5f2fae67e8cb6d60c76f\n",
      "  Stored in directory: /home/ucloud/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
      "Successfully built pyahocorasick\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.2.0 contractions-0.0.49 pyahocorasick-1.4.2 textsearch-0.0.21\n",
      "Tensorflow Version:  2.5.0\n"
     ]
    }
   ],
   "source": [
    "#@title Libraries\n",
    "!pip install tensorflow\n",
    "!pip install nltk\n",
    "!pip install contractions\n",
    "#nltk.download('punkt')\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku\n",
    "import contractions\n",
    "\n",
    "print('Tensorflow Version: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "yfnSPZQZe0yv"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>album</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>intro</th>\n",
       "      <th>verse</th>\n",
       "      <th>chorus</th>\n",
       "      <th>bridge</th>\n",
       "      <th>outro</th>\n",
       "      <th>intro_cleaned</th>\n",
       "      <th>verse_cleaned</th>\n",
       "      <th>bridge_cleaned</th>\n",
       "      <th>chorus_cleaned</th>\n",
       "      <th>outro_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Let It Be</td>\n",
       "      <td>May 8, 1970</td>\n",
       "      <td>Let It Be</td>\n",
       "      <td>\\r\\n\\r\\n[Verse 1]\\r\\nWhen I find myself in tim...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['[Verse 1]\\nWhen I find myself in times of tr...</td>\n",
       "      <td>['[Chorus]\\nLet it be, let it be, let it be, l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>when i find myself in times of trouble, mothe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>let it be, let it be, let it be, let it be wh...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Come Together</td>\n",
       "      <td>September 26, 1969</td>\n",
       "      <td>Abbey Road</td>\n",
       "      <td>\\r\\n\\r\\n[Intro]\\r\\nShoot me\\r\\nShoot me\\r\\nSho...</td>\n",
       "      <td>['[Intro]\\nShoot me\\nShoot me\\nShoot me\\nShoot...</td>\n",
       "      <td>[\"[Verse 1]\\nHere come old flat-top, he come g...</td>\n",
       "      <td>['[Chorus]\\nCome together, right now\\nOver me'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['[Outro]\\nCome together, yeah\\nCome together,...</td>\n",
       "      <td>shoot me shoot me shoot me shoot me'</td>\n",
       "      <td>here come old flat-top, he come groovin' up s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>come together, right now over me', come toget...</td>\n",
       "      <td>come together, yeah come together, yeah come ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Yesterday</td>\n",
       "      <td>September 13, 1965</td>\n",
       "      <td>Help! (UK)</td>\n",
       "      <td>\\r\\n\\r\\n[Verse 1]\\r\\nYesterday\\r\\nAll my troub...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[\"[Verse 1]\\nYesterday\\nAll my troubles seemed...</td>\n",
       "      <td>[\"[Chorus]\\nWhy she had to go\\nI don't know, s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['[Outro]\\nMmm-mmm-mmm-mmm-mmm, hmm-hmm']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yesterday all my troubles seemed so far away ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>why she had to go i don't know, she wouldn't ...</td>\n",
       "      <td>mmm-mmm-mmm-mmm-mmm, hmm-hmm'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Something</td>\n",
       "      <td>September 26, 1969</td>\n",
       "      <td>Abbey Road</td>\n",
       "      <td>\\r\\n\\r\\n[Verse 1]\\r\\nSomething in the way she ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['[Verse 1]\\nSomething in the way she moves\\nA...</td>\n",
       "      <td>[\"[Chorus]\\nI don't want to leave her now\\nYou...</td>\n",
       "      <td>[\"[Bridge]\\nYou're asking me will my love grow...</td>\n",
       "      <td>[\"[Outro]\\nI don't want to leave her now\\nYou ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>something in the way she moves attracts me li...</td>\n",
       "      <td>you're asking me will my love grow i don't kn...</td>\n",
       "      <td>i don't want to leave her now you know i beli...</td>\n",
       "      <td>i don't want to leave her now you know i beli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Here Comes the Sun</td>\n",
       "      <td>September 26, 1969</td>\n",
       "      <td>Abbey Road</td>\n",
       "      <td>\\r\\n\\r\\n[Chorus]\\r\\nHere comes the sun, doo da...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[\"[Verse 1]\\nLittle darling, it's been a long ...</td>\n",
       "      <td>[\"[Chorus]\\nHere comes the sun, doo da doo doo...</td>\n",
       "      <td>['[Bridge]\\nSun, sun, sun, here it comes\\nSun,...</td>\n",
       "      <td>[\"[Outro]\\nHere comes the sun, doo da doo doo\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>little darling, it's been a long cold lonely ...</td>\n",
       "      <td>sun, sun, sun, here it comes sun, sun, sun, h...</td>\n",
       "      <td>here comes the sun, doo da doo doo here comes...</td>\n",
       "      <td>here comes the sun, doo da doo doo here comes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1               title                date  \\\n",
       "0           0             0           Let It Be         May 8, 1970   \n",
       "1           1             1       Come Together  September 26, 1969   \n",
       "2           2             2           Yesterday  September 13, 1965   \n",
       "3           3             3           Something  September 26, 1969   \n",
       "4           4             4  Here Comes the Sun  September 26, 1969   \n",
       "\n",
       "        album                                             lyrics  \\\n",
       "0   Let It Be  \\r\\n\\r\\n[Verse 1]\\r\\nWhen I find myself in tim...   \n",
       "1  Abbey Road  \\r\\n\\r\\n[Intro]\\r\\nShoot me\\r\\nShoot me\\r\\nSho...   \n",
       "2  Help! (UK)  \\r\\n\\r\\n[Verse 1]\\r\\nYesterday\\r\\nAll my troub...   \n",
       "3  Abbey Road  \\r\\n\\r\\n[Verse 1]\\r\\nSomething in the way she ...   \n",
       "4  Abbey Road  \\r\\n\\r\\n[Chorus]\\r\\nHere comes the sun, doo da...   \n",
       "\n",
       "                                               intro  \\\n",
       "0                                                 []   \n",
       "1  ['[Intro]\\nShoot me\\nShoot me\\nShoot me\\nShoot...   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                               verse  \\\n",
       "0  ['[Verse 1]\\nWhen I find myself in times of tr...   \n",
       "1  [\"[Verse 1]\\nHere come old flat-top, he come g...   \n",
       "2  [\"[Verse 1]\\nYesterday\\nAll my troubles seemed...   \n",
       "3  ['[Verse 1]\\nSomething in the way she moves\\nA...   \n",
       "4  [\"[Verse 1]\\nLittle darling, it's been a long ...   \n",
       "\n",
       "                                              chorus  \\\n",
       "0  ['[Chorus]\\nLet it be, let it be, let it be, l...   \n",
       "1  ['[Chorus]\\nCome together, right now\\nOver me'...   \n",
       "2  [\"[Chorus]\\nWhy she had to go\\nI don't know, s...   \n",
       "3  [\"[Chorus]\\nI don't want to leave her now\\nYou...   \n",
       "4  [\"[Chorus]\\nHere comes the sun, doo da doo doo...   \n",
       "\n",
       "                                              bridge  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3  [\"[Bridge]\\nYou're asking me will my love grow...   \n",
       "4  ['[Bridge]\\nSun, sun, sun, here it comes\\nSun,...   \n",
       "\n",
       "                                               outro  \\\n",
       "0                                                 []   \n",
       "1  ['[Outro]\\nCome together, yeah\\nCome together,...   \n",
       "2          ['[Outro]\\nMmm-mmm-mmm-mmm-mmm, hmm-hmm']   \n",
       "3  [\"[Outro]\\nI don't want to leave her now\\nYou ...   \n",
       "4  [\"[Outro]\\nHere comes the sun, doo da doo doo\\...   \n",
       "\n",
       "                           intro_cleaned  \\\n",
       "0                                    NaN   \n",
       "1   shoot me shoot me shoot me shoot me'   \n",
       "2                                    NaN   \n",
       "3                                    NaN   \n",
       "4                                    NaN   \n",
       "\n",
       "                                       verse_cleaned  \\\n",
       "0   when i find myself in times of trouble, mothe...   \n",
       "1   here come old flat-top, he come groovin' up s...   \n",
       "2   yesterday all my troubles seemed so far away ...   \n",
       "3   something in the way she moves attracts me li...   \n",
       "4   little darling, it's been a long cold lonely ...   \n",
       "\n",
       "                                      bridge_cleaned  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3   you're asking me will my love grow i don't kn...   \n",
       "4   sun, sun, sun, here it comes sun, sun, sun, h...   \n",
       "\n",
       "                                      chorus_cleaned  \\\n",
       "0   let it be, let it be, let it be, let it be wh...   \n",
       "1   come together, right now over me', come toget...   \n",
       "2   why she had to go i don't know, she wouldn't ...   \n",
       "3   i don't want to leave her now you know i beli...   \n",
       "4   here comes the sun, doo da doo doo here comes...   \n",
       "\n",
       "                                       outro_cleaned  \n",
       "0                                                NaN  \n",
       "1   come together, yeah come together, yeah come ...  \n",
       "2                      mmm-mmm-mmm-mmm-mmm, hmm-hmm'  \n",
       "3   i don't want to leave her now you know i beli...  \n",
       "4   here comes the sun, doo da doo doo here comes...  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_dataset.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxelBhzWP2Vt"
   },
   "source": [
    "**Clean Lyrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "cellView": "form",
    "id": "8n1s_OpMu2Yb"
   },
   "outputs": [],
   "source": [
    "#@title Clean Lyrics\n",
    "def clean(s):\n",
    "    \n",
    "    # Expand contractions\n",
    "    s = s.split()\n",
    "    new_s = []\n",
    "    for word in s:\n",
    "        new_s.append(contractions.fix(word))\n",
    "    s = \" \".join(new_s)\n",
    "\n",
    "    # Format words and remove unwanted characters\n",
    "    s = re.sub(r'[\\(\\[].*?[\\)\\]] +', '', s)\n",
    "    s = re.sub(r' [\\(\\[].*?[\\)\\]]', '', s)\n",
    "    s = re.sub(r\"[^’'a-zA-Z \\ ]+\", ' ', s)\n",
    "    s = re.sub(' +', ' ', s)\n",
    "    s = s.lower()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "PMJyTYOwA9pH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i am sure to fall fall in love i am sure to fa...\n",
       "Name: test, dtype: object"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new colunn with clean lyrics\n",
    "df['lyrics_cleaned'] = list(map(clean, df.lyrics))\n",
    "df['test'] = df['lyrics_cleaned'][310:311]\n",
    "df['lyrics_cleaned'] = df['lyrics_cleaned'][0:310]\n",
    "lyrics = df['lyrics_cleaned'].dropna()\n",
    "test = df['test'].dropna()\n",
    "test.reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "cellView": "form",
    "id": "rmpInsN2B_fB"
   },
   "outputs": [],
   "source": [
    "#@title Extract Text\n",
    "# extract all from lyrics cleaned col to string\n",
    "# todo: regex remove all stopchars, puncation etc, except: [Intro], [Verse], [Chorus], hence remove verse number ([Verse 1])\n",
    "def extract_text(lyrics):\n",
    "    global corpus\n",
    "    lst = []\n",
    "    for i in range(len(lyrics)):\n",
    "        lst.append(str(lyrics[i]))\n",
    "    data = ''.join(lst)\n",
    "    \n",
    "    corpus = []\n",
    "    corpus += [w for w in data.split(' ') if w.strip() != '' or w == '\\n']\n",
    "    lyrics = ' '.join(corpus)\n",
    "    \n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ar3vlhG2CBCe",
    "outputId": "2166f70c-5be0-406e-9ee8-cf5befd8dabc"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Expand contractions\n",
    "def expand(s): \n",
    "    s = s.lower()\n",
    "    s = s.split()\n",
    "    new_s = []\n",
    "    for word in s:\n",
    "        new_s.append(contractions.fix(word))\n",
    "    s = \" \".join(new_s)\n",
    "    return s\n",
    "\n",
    "lyrics = extract_text(lyrics)\n",
    "#lyrics = expand(lyrics)\n",
    "intro = extract_text(df['intro_cleaned'])\n",
    "#intro = expand(intro)\n",
    "verse = extract_text(df['verse_cleaned'])\n",
    "#verse = expand(verse)\n",
    "chorus = extract_text(df['chorus_cleaned'])\n",
    "#chorus = expand(chorus)\n",
    "bridge = extract_text(df['bridge_cleaned'])\n",
    "#bridge = expand(bridge)\n",
    "outro = extract_text(df['outro_cleaned'])\n",
    "#outro = expand(outro)\n",
    "test = extract_text(test)\n",
    "#test = expand(test)\n",
    "\n",
    "# most common words from string\n",
    "Counter_intro = Counter(intro.split())\n",
    "most_occur_intro = Counter_intro.most_common(10)\n",
    "\n",
    "Counter_verse = Counter(verse.split())\n",
    "most_occur_verse = Counter_verse.most_common(20)\n",
    "\n",
    "Counter_chorus = Counter(chorus.split())\n",
    "most_occur_chorus = Counter_chorus.most_common(20)\n",
    "\n",
    "Counter_bridge = Counter(bridge.split())\n",
    "most_occur_bridge = Counter_bridge.most_common(20)\n",
    "\n",
    "Counter_outro = Counter(outro.split())\n",
    "most_occur_outro = Counter_outro.most_common(10)\n",
    "\n",
    "#random.choice(most_occur_verse)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bmloyk3fP5m3"
   },
   "source": [
    "**Get Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "4p4iBYgh19QA"
   },
   "outputs": [],
   "source": [
    "# Choose characted based or word based text generation\n",
    "char_ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sshbGjbKwrQ9",
    "outputId": "21a0dd97-2b77-446e-834a-687ac0f9eb49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 unique characters in vocabulary\n",
      "<class 'str'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#@title Get Vocabulary\n",
    "# vocab char/word [T/F] (default = T)\n",
    "def vocab(lyrics, char = char_): #might do global char to specify type in beginning\n",
    "    if char == True:\n",
    "        vocab = sorted(set(lyrics))\n",
    "        print(f'{len(vocab)} unique characters in vocabulary')\n",
    "    else:\n",
    "        lyrics = [w for w in lyrics.split(' ')]\n",
    "        vocab = sorted(set(lyrics))\n",
    "        print(f'{len(vocab)} unique words in vocabulary')\n",
    "    return lyrics, vocab\n",
    "\n",
    "lyrics, vocab = vocab(lyrics)\n",
    "\n",
    "print(type(lyrics)); print(type(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjLKKhk3QJBR"
   },
   "source": [
    "**Vectorize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "cellView": "form",
    "id": "ML-xuiXwQZ9X"
   },
   "outputs": [],
   "source": [
    "#@title Vectorize\n",
    "# vec char/word [T/F] (default = T)\n",
    "## keep this structure for now, reduce later\n",
    "def vec_data(vocab, lyrics, char = char_):\n",
    "    if char == True: \n",
    "        print('character-based')\n",
    "        char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "        idx2char = np.array(vocab)\n",
    "        text_as_int = np.array([char2idx[c] for c in lyrics])\n",
    "        return char2idx, idx2char, text_as_int\n",
    "\n",
    "    else:\n",
    "        word2idx = {u:i for i, u in enumerate(vocab)}\n",
    "        print('word-based')\n",
    "        idx2words = np.array(vocab)\n",
    "        word_as_int = np.array([word2idx[w] for w in lyrics])\n",
    "        return word2idx, idx2words, word_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yPrUfiQyWsDh",
    "outputId": "c4c77f1a-cb45-4b23-847f-73ef63fb7bcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character-based\n",
      "<class 'dict'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "29\n",
      "29\n",
      "328295\n"
     ]
    }
   ],
   "source": [
    "# call based on char [T/F]\n",
    "cw_2idx, idx2_cw, as_int = vec_data(vocab, lyrics, char = char_)\n",
    "\n",
    "#%%\n",
    "print(type(cw_2idx)); print(type(idx2_cw)); print(type(as_int))\n",
    "print(len(cw_2idx)); print(len(idx2_cw)); print(len(as_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YkL4qNYc1uL",
    "outputId": "f97169e4-283b-45f8-bf49-102d93f795e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length set to: 100\n",
      "Examples per epoch set to: 3250\n"
     ]
    }
   ],
   "source": [
    "#@title Set Sequence\n",
    "#define sequence\n",
    "def seq(lyrics = lyrics, char = char_):\n",
    "    global seq_length\n",
    "    global examples_per_epoch\n",
    "    if char == True:\n",
    "        seq_length = 100\n",
    "        examples_per_epoch = len(lyrics)//(seq_length+1)\n",
    "    else: \n",
    "        seq_length = 10\n",
    "        examples_per_epoch = len(corpus)//(seq_length+1)\n",
    "\n",
    "#call seq\n",
    "seq()\n",
    "\n",
    "print(f'Sequence length set to: {seq_length}')\n",
    "print(f'Examples per epoch set to: {examples_per_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SKAXaibk7zk"
   },
   "source": [
    "**Set Model Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "G7T0x-8Jh31Q"
   },
   "outputs": [],
   "source": [
    "cw_dataset = tf.data.Dataset.from_tensor_slices(as_int)\n",
    "sequences = cw_dataset.batch(seq_length+1, drop_remainder=True) # generating batches of 10 words each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "M1zUK5N0h4kG"
   },
   "outputs": [],
   "source": [
    "#further preprossing (duplicate and shift one step below) (helper function)\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zf6s0vVnisv5",
    "outputId": "7ee1c9b1-310d-45b1-d2fe-6ff26f2d7101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# define batch & buffer size\n",
    "batch_size = 64\n",
    "buffer_size = 100 #this should be changed with char/word, consider putting it in parameter function\n",
    "\n",
    "steps_per_epoch =  len(lyrics) // batch_size\n",
    "\n",
    "#shuffle data\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True) #.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# set parameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "# check shapes\n",
    "print(dataset) # should be ((int, int), (int, int)), else run from top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw65MW1Xm0e0"
   },
   "source": [
    "**Create Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "S6dzqp93y5r2"
   },
   "outputs": [],
   "source": [
    "def createModel(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMCBB4NOBTYY",
    "outputId": "901056e1-b7c7-4cd0-bf5a-74ac60868bdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Character based Model: \n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (64, None, 256)           7424      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 29)            29725     \n",
      "=================================================================\n",
      "Total params: 3,975,453\n",
      "Trainable params: 3,975,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if char_ == True:\n",
    "    model_char = createModel(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)\n",
    "    print('Character based Model: ')\n",
    "    model_char.summary()\n",
    "else:\n",
    "    model_word = createModel(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)\n",
    "    print('Word based Model: ')\n",
    "    model_word.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trlmfmX_oY51"
   },
   "source": [
    "**Set Loss Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "XKbJZF38z1Ou",
    "outputId": "c261998a-f3ee-49d8-eb23-a7211f44cdbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 41s 795ms/step - loss: 3.0889\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 40s 798ms/step - loss: 2.2426\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 40s 792ms/step - loss: 2.0438\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 39s 789ms/step - loss: 1.9157\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 40s 792ms/step - loss: 1.8066\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 40s 797ms/step - loss: 1.7079\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 40s 795ms/step - loss: 1.6195\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 39s 789ms/step - loss: 1.5404\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 40s 800ms/step - loss: 1.4671\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 40s 793ms/step - loss: 1.4012\n",
      "400.54 seconds running time for char-based model\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "\n",
    "if char_ == False:\n",
    "    EPOCHS = 20\n",
    "    start_word = time.time()\n",
    "    model_word.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "    #save training checkpoints\n",
    "    checkpoint_dir = './training_checkpoints_word'\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "    checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_prefix,\n",
    "        save_weights_only=True)\n",
    "    \n",
    "    history = model_word.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "    tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    model_word = create_model(len(vocab), embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "    model_word.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    model_word.build(tf.TensorShape([1, None]))\n",
    "    end_word = time.time()\n",
    "    print(round(end_word-start_word, 2), 'seconds running time for word-based model')\n",
    "    \n",
    "    \n",
    "\n",
    "elif char_ == True:\n",
    "    \n",
    "    EPOCHS = 10\n",
    "    start_char = time.time()\n",
    "    model_char.compile(optimizer='adam', loss=loss)\n",
    "    \n",
    "    #save training checkpoints\n",
    "    checkpoint_dir = './training_checkpoints_char'\n",
    "    \n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "    checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_prefix,\n",
    "        save_weights_only=True)\n",
    "    \n",
    "    history = model_char.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "    tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    model_char = createModel(len(vocab), embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "    model_char.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    model_char.build(tf.TensorShape([1, None]))\n",
    "    end_char = time.time()\n",
    "    print(round(end_char-start_char, 2), 'seconds running time for char-based model')\n",
    "\n",
    "else: \n",
    "    print('Set char_ to T/F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "M0Qd_SI-9sk7",
    "outputId": "8ee9e2cf-b615-46a2-e555-1b4d2aa87086"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e4f03981d906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#save trained model for future use (so we do not have to train it every time we want to generate text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Example:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerateLyrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartString\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu\"love\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_word' is not defined"
     ]
    }
   ],
   "source": [
    "def generateLyrics(model, startString, temp, num_generate = 30):\n",
    "    #print(\"---- Generating lyrics starting with '\" + startString + \"' ----\")\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    start_string_list =  [w for w in startString.split(' ')]\n",
    "    input_eval = [cw_2idx[s] for s in start_string_list]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # temp represent how 'conservative' the predictions are. \n",
    "        # Lower temp leads to more predictable (or correct) lyrics\n",
    "        predictions = predictions / temp \n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(' ' + idx2_cw[predicted_id])\n",
    "\n",
    "    return (startString + ''.join(text_generated))\n",
    "\n",
    "#save trained model for future use (so we do not have to train it every time we want to generate text)\n",
    "model_word.save('saved_model.h5') \n",
    "print(\"Example:\")\n",
    "print(generateLyrics(model_word, startString=u\"love\", temp=0.6))\n",
    "while (True):\n",
    "    print('Enter start string:')\n",
    "    input_str = input().lower().strip()\n",
    "    print('Enter temp:')\n",
    "    temp = float(input())\n",
    "    print(generateLyrics(model_word, startString=input_str, temp=temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_song(format,model):\n",
    "    \n",
    "    temp = 0.6\n",
    "    song = []\n",
    "    \n",
    "    if model == model_char:\n",
    "        a,b,c,d,e = 200,300,300,200,300\n",
    "        generator = char_based_generator\n",
    "    else: \n",
    "        a,b,c,d,e = 20,30,30,20,30\n",
    "        generator = generateLyrics\n",
    "        \n",
    "    intro = generator(model_char, startString=random.choice(most_occur_intro)[0], temp=temp, num_generate = a)\n",
    "    chorus = generator(model_char, startString=random.choice(most_occur_chorus)[0], temp=temp, num_generate = b)\n",
    "    bridge = generator(model_char, startString=random.choice(most_occur_bridge)[0], temp=temp, num_generate = c)\n",
    "    outro = generator(model_char, startString=random.choice(most_occur_outro)[0], temp=temp, num_generate = d)\n",
    "    \n",
    "    for tag in format:\n",
    "        if tag == \"I\":\n",
    "            print('[INTRO]\\n',intro,'\\n')\n",
    "        elif tag == \"V\":\n",
    "            verse = generator(model_char, startString=random. choice(most_occur_verse)[0], temp=temp, num_generate = e)\n",
    "            print('[VERSE]\\n',verse,'\\n')\n",
    "        elif tag == \"B\":\n",
    "            print('[BRIDGE]\\n',bridge,'\\n')\n",
    "        elif tag == \"C\":\n",
    "            print('[CHORUS]\\n',chorus,'\\n')\n",
    "        elif tag == \"O\":\n",
    "            print('[OUTRO]\\n',outro)\n",
    "        else: \n",
    "            raise ValueError('wrong input')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INTRO]\n",
      " you here home in am we are going to carry you want me the wait. i cull enged me to you the waiting but ev'rybody do not you know i think it is go on the benthow but i am not no don ever have here all you \n",
      "\n",
      "[VERSE]\n",
      " is bempmony lead me the wanter have been plone to the you one sweet donn and i do not know what a said make me starting then it all got to know something rack of the world do not want me say he will have here it is not the end of the sun and in the meanie conds. we all gotether now a to calling of bea \n",
      "\n",
      "[CHORUS]\n",
      " to you do not know my love to the thought the world the party called me no i no not no do not know know show my knee when you can not you to be a uppering to the singers off i need so i will not the pain out a seace the ring back to well you will get you one can not going to the way a pain easter you  \n",
      "\n",
      "[VERSE]\n",
      " a hand masic finger dim not every the ligsten but a pooplow now i need it is come so hall hand not girl you know that i have and i do not want to be a people people end the.. i am going to you you better help me well pred we have been gone that i me mine there is you want me to lose that girl you wou \n",
      "\n",
      "[BRIDGE]\n",
      " that man the was the window und you know the new yeah yeah yeah again. every happy you are going to lose that said so i can not you know i need you being around help me no do not say that. me i go and rown of the sund of the world if you so if i am said you sill but she have got me help me well i love y \n",
      "\n",
      "[CHORUS]\n",
      " to you do not know my love to the thought the world the party called me no i no not no do not know know show my knee when you can not you to be a uppering to the singers off i need so i will not the pain out a seace the ring back to well you will get you one can not going to the way a pain easter you  \n",
      "\n",
      "[VERSE]\n",
      " not before that are going to change my mman back he is got to to do do do do do do do do do do do do do look at the nawfer number time it is near you see you bet me got to go so i got to ride she ought to fide a lous and make me you are got a ticket the lonely pait and i will never weep to go i am goin \n",
      "\n",
      "[OUTRO]\n",
      " ally heart if you till the way my love will baby not know how to the hands in a yeah oh yeah you know my nane had a was a gir me to the ring a point of how i need as to make you love to to site to help m\n"
     ]
    }
   ],
   "source": [
    "song = make_song(\"IVCVBCVO\",model_char)\n",
    "song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSE0e_GDpCwb"
   },
   "source": [
    "**Lyric Generators**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TCI06my7KkUT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "hey happy we want me so in. it ev\n"
     ]
    }
   ],
   "source": [
    "# The prediction loop\n",
    "def char_based_generator(model, startString, temp, num_generate = 30):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    " \n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [cw_2idx[s] for s in startString]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = temp\n",
    " \n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2_cw[predicted_id])\n",
    " \n",
    "    return (startString + ''.join(text_generated))\n",
    "model_char.save('char_based_model.h5') \n",
    "print(char_based_generator(model_char, startString=u\"hey\",temp = 0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_loss_by_example(logits,\n",
    "                             targets,\n",
    "                             weights,\n",
    "                             average_across_timesteps=True,\n",
    "                             softmax_loss_function=None,\n",
    "                             name=None):\n",
    "  \"\"\"Weighted cross-entropy loss for a sequence of logits (per example).\n",
    "  Args:\n",
    "    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n",
    "    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n",
    "    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n",
    "    average_across_timesteps: If set, divide the returned cost by the total\n",
    "      label weight.\n",
    "    softmax_loss_function: Function (labels-batch, inputs-batch) -> loss-batch\n",
    "      to be used instead of the standard softmax (the default if this is None).\n",
    "    name: Optional name for this operation, default: \"sequence_loss_by_example\".\n",
    "  Returns:\n",
    "    1D batch-sized float Tensor: The log-perplexity for each sequence.\n",
    "  Raises:\n",
    "    ValueError: If len(logits) is different from len(targets) or len(weights).\n",
    "  \"\"\"\n",
    "      if len(targets) != len(logits) or len(weights) != len(logits):\n",
    "          raise ValueError(\"Lengths of logits, weights, and targets must be the same \"\n",
    "                     \"%d, %d, %d.\" % (len(logits), len(weights), len(targets)))\n",
    "    with ops.name_scope(name, \"sequence_loss_by_example\",\n",
    "                      logits + targets + weights):\n",
    "    log_perp_list = []\n",
    "    for logit, target, weight in zip(logits, targets, weights):\n",
    "      if softmax_loss_function is None:\n",
    "        # TODO(irving,ebrevdo): This reshape is needed because\n",
    "        # sequence_loss_by_example is called with scalars sometimes, which\n",
    "        # violates our general scalar strictness policy.\n",
    "        target = array_ops.reshape(target, [-1])\n",
    "        crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=target, logits=logit)\n",
    "      else:\n",
    "        crossent = softmax_loss_function(target, logit)\n",
    "      log_perp_list.append(crossent * weight)\n",
    "    log_perps = math_ops.add_n(log_perp_list)\n",
    "    if average_across_timesteps:\n",
    "      total_size = math_ops.add_n(weights)\n",
    "      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n",
    "      log_perps /= total_size\n",
    "  return log_perps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset is equal to test from the beginning.\n",
    "#this should work for n-grams. \n",
    "#need a big dictionary with the probability for each word. aka. model[word]\n",
    "\n",
    "def perplexity(testset, model)\n",
    "    testset = testset.split()\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for word in testset:\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/model[word])\n",
    "    perplexity = pow(perplexity, 1/float(N)) \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-244-93ea7c4ed4cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "logits = keras.layers.Dense(10)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fbc9f0f2c10>) with an unsupported type (<class 'tensorflow.python.keras.engine.sequential.Sequential'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-a36a8b53281c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_v2\u001b[0;34m(logits, axis, name)\u001b[0m\n\u001b[1;32m   3699\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3700\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3701\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_wrap_2d_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_wrap_2d_function\u001b[0;34m(inputs, compute_op, dim, name)\u001b[0m\n\u001b[1;32m   3611\u001b[0m         name=name)\n\u001b[1;32m   3612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3613\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m   \u001b[0;31m# We need its original shape for shape inference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fbc9f0f2c10>) with an unsupported type (<class 'tensorflow.python.keras.engine.sequential.Sequential'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "probabilities = tf.nn.softmax(model_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Sequential' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-cf2e352e96e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-234-f15018ce15b4>\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(testset, model)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperplexity\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Sequential' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "perplexity(test, model_char)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BLG.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
